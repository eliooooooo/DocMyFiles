// Dependencies
import { fileSync, writeFileSync, promises, readdirSync, readFileSync, statSync, promisify, exec, fileURLToPath, join, dirname, __filename, __dirname, config, OpenAI, openai, chalk, rl } from './dependencies.js';

// Import the custom functions from the project 
import { countTokens, askQuestion, processFile, processDirectory, fileStack, messageList, processMessage } from './functions.js';

// Check your OpenAI account to get your tier rate
// The tier rate affect the number of tokens you can send in a minute, it's important to customize it to get a faster result
const tierRate = {
	"Tier 1": {
		"tpm": 60000
	},"Tier 2": {
		"tpm": 80000
	},"Tier 3": {
		"tpm": 160000
	},"Tier 4": {
		"tpm": 1000000
	},"Tier 5": {
		"tpm": 2000000
	},"custom": {
		"tpm": "custom value"
	}
}

// *------------------------------------*
// |                                    |
// |         CUSTOMIZE VARIABLES        |
// |                                    |
// *------------------------------------*
const enableTokenizer = true; // enable it will execute a python script to count the tokens, make sure to have python installed

const openaiTier = "Tier 1"; // Don't forget to custom the openai tier, it refers to the tierRate object
const projectPath = './project/WriteMyCommits/'; // Don't forget to custom the project path
const avoid = ['.git', 'icons', 'package-lock.json', 'composer.lock', '.vscode' ]; // Don't forget to custom the avoid table to avoid some files or directories
const description = 'A vscode extension to generate conventionnals commits based on user inputs.'; // Don't forget to custom the description of your project

// Calculate the number of tokens in the description
// Don't modify this part
const TOKENS_DESCRIPTION = await countTokens([{ role: 'system', content: description }]);


// *------------------------------------*
// |                                    |
// |           GLOBAL VARIABLES         |
// |                                    |
// *------------------------------------*
let contextFiles = [];
const TOKENS_PER_MINUTES = tierRate[openaiTier].tpm - 1500;
const MAX_TOKENS = 15000;
let messages = [
	{ role: 'system', content: 'You are a useful assistant, specialized in programming. You\'re mainly used to generate custom readme files. Here is a short description of my project : ' + description + '. Here are my project files so that you can generate a custom README for me :' }
];

// ! The result is not consistent with theses prompts.
// ! Some of the prompts are not returning full reports
// ! For little projects, the result is not consistent, + review on the price or anything else that does not send the messages
// ? Use cont tokens functions ?
// Classic instruction to generate a readme for a little project
const CLASSIC_MESSAGE = { role: 'system', content: 'You are a useful assistant, specialized in programming. You\'re mainly used to generate custom readme files. Here is a short description of my project : ' + description + '. Here are my project files so that you can generate a custom README for me :' };
const TOKENS_CLASSIC_MESSAGE = await countTokens([CLASSIC_MESSAGE]) + TOKENS_DESCRIPTION;

// Classic instruction to generate a readme for a big project
const BIG_MESSAGE = { role: 'system', content: 'You are a useful assistant, specialized in programming. You\'re mainly used to generate custom readme files for projects. Here is a short description of my project : ' + description + '. I want to send you multiple requests with each multiple files. Please generate a full report of the files so later you can generate a README from multiple report files.' };
const TOKENS_BIG_MESSAGE = await countTokens([BIG_MESSAGE]) + TOKENS_DESCRIPTION;

// ? give an exmple of a big project readme ?
// Last instruction to generate the readme for big project
const LAST_BIG_MESSAGE = { role: 'system', content: 'You are a useful assistant, specialized in programming. You\'re mainly used to generate custom readme files for projects. Here is a short description of my project : ' + description + '. I have sent you multiple requests with each multiple files and you have generate multiple full report from these requests. Please generate a README based on my description and your full reports.' };
const TOKENS_LAST_BIG_MESSAGE = countTokens([LAST_BIG_MESSAGE]) + TOKENS_DESCRIPTION;


// *------------------------------------*
// |                                    |
// |           FUNCTIONS                |
// |                                    |
// *------------------------------------*




/**
 * Send the request to OpenAI
 * @param {string} projectPath
 * @param {string[]} messages
 * 
 * @returns {void}
 */
async function sendRequest(projectPath, messagesList) {
	const footer = "<br><br> This README was generated by [DocMyFiles](https://github.com/eliooooooo/DocMyFiles).";
	let bigRequest = false;
	let longRequest = false;
	let requestSize = 0;

	if (enableTokenizer) {
		// Collect the messages in a temporary file
		let tmpMessagesList = fileSync();
		writeFileSync(tmpMessagesList.name, JSON.stringify(messagesList)); 

		// Count the number of tokens in the message with a python script
		let stdout = (await exec(`python3 tokenCounter.py "${tmpMessagesList.name}"`)).stdout;
		requestSize = Number(stdout);
		
		// The price of the request + his firts instruction if it's not too big
		const estimatedClassicPrice = requestSize + TOKENS_CLASSIC_MESSAGE;

		// Check if the request is too big 
		// Display a warning message if it's the case
		if (estimatedClassicPrice > MAX_TOKENS ) {
			// Update request size variables
			if ((estimatedClassicPrice) > TOKENS_PER_MINUTES) longRequest = true;
			bigRequest = true;
			
			// Display a warning message
			// ? Deplace this part in a function
			// TODO: Move the estimated time just before sending the request
			console.log("");
			console.log(chalk.red('/---------------------------------------------------------------------------\\'));
			console.log(chalk.red('Your request is too big, the request will be send in multiple parts.'));
			if (longRequest) {
				console.log(chalk.red('You are using the ' + openaiTier + ' with a tpm (tokens per minute) of ' + TOKENS_PER_MINUTES + ' tokens'));
				console.log(chalk.red('To respect this restriction, requests will be delayed.' + chalk.bold(' Estimated time : ' + Math.ceil(requestSize/TOKENS_PER_MINUTES) + ' minutes')));
			}
			console.log(chalk.red('Please make sure you have correctly customize the avoid table in the script.'))
			console.log(chalk.red('\\--------------------------------------------------------------------------/'));
		} 

		// ? Deplace here the estimated number of requests. So we can estimate more precisely the cost for a big request
		// TODO: Deplace it

		// Display the estimated price of the request
		let estimatedPrice = (estimatedClassicPrice/1000)*0.0005;
		let price = estimatedClassicPrice + ' ( ' + chalk.red('+- ' + estimatedPrice.toFixed(3) + ' $') + ' )';
		console.log(chalk.bold('Estimated tokens price : '), price);
		console.log("");
	}

	// Ask the user if he want to send the request
	const answer = await askQuestion('Do you want to send the request to OpenAI ? ' + chalk.gray('(yes/no) [yes] '));
	console.log('');

	// if user want to send the requests
	if (answer.toLowerCase() === 'yes' ||  answer === ''){
		// if the request is too big
		if (bigRequest) {
			// Initialize variables for the request and overview
			let listRequest = [];
			let listRequestMessages = [	BIG_MESSAGE	];
			let listMessagesSize = 0;
			let totalMessages = 0;
			let ignoredFiles = 0;
			
			// Define the number of request to send
			let estimatedNumber = Math.ceil(requestSize/MAX_TOKENS);
			console.log(chalk.cyan('Request will be send in approx. ' + estimatedNumber + ' parts.'));
			console.log(chalk.cyan('Parsing request...'));
			console.log('');
			console.log('---------------');

			// Processing all messages to split them into several requests
			for (let message of messagesList) {
				({ listRequest, listRequestMessages, listMessagesSize, totalMessages, ignoredFiles } = await processMessage(message, listRequest, listRequestMessages, listMessagesSize, totalMessages, ignoredFiles));
			};

			// Push the last request and update overview variables
			listRequest.push(listRequestMessages);
			console.log(chalk.magenta('Request added to the list with ' + listRequestMessages.length + ' messages ' + '( ' + listRequest.length + ' requests )'));
			totalMessages += listRequestMessages.length;
				
			// Display the overview of the request
			console.log("---------------");
			console.log("");
			console.log(chalk.bold(chalk.green("Parsing done ! ")) + "Overview of the request : ");
			console.log('Request parsed in ' + chalk.cyan(listRequest.length + ' parts.'));
			console.log('With a total of ' + chalk.cyan(totalMessages + ' messages.'));
			console.log(chalk.red(ignoredFiles) + ' files have been ignored because they were too big.');
			console.log("");

			// Ask the user if he want to send the requests
			const answer = await askQuestion('Do you want to send the requests to OpenAI ? ' + chalk.gray('(yes/no) [yes] '));
			console.log("");
			
			// if user want to send the request
			if (answer.toLowerCase() === 'yes' ||  answer === ''){
				// Initialize variables for the request
				let i = 1;
				let tokensUsed = 0;
				let price = 0;
				let requestPerMin = Math.ceil(TOKENS_PER_MINUTES/MAX_TOKENS);

				// Display the estimated time for longs requests
				if (longRequest) {
					console.log(chalk.bold('Requests per minute : ' + chalk.cyan(requestPerMin)));
					console.log(chalk.bold('Estimated time : ' + chalk.cyan(Math.ceil(listRequest.length/requestPerMin) + ' minutes')));
				}
				console.log("------------------");

				// Send the requests to OpenAI
				for (let request of listRequest) {
					console.log(chalk.magenta('Sending request ' + i + ' for processing...'));

					// Sending the request to OpenAI
					const response = await openai.chat.completions.create({
						model: 'gpt-3.5-turbo',
						messages: request
					});

					// Update the overview variables
					tokensUsed += Number(response.usage.total_tokens);
					price += Number((tokensUsed/1000)*0.0005);

					// Write the full report in a temporary file and add the url to the contextFiles
					let tmpReport = fileSync();
					writeFileSync(tmpReport.name, JSON.stringify(response.choices[0].message.content));
					contextFiles.push(tmpReport.name);

					// ! Move this part in a function
					// If it's the last request, write the README and display the overview
					// If it's a long request, wait 1 minute to respect the tpm
					if (i === listRequest.length) {
						// Initialize variables for the last request
						let lastRequest = [];
						let contextRequestSize = 0;
						let i = 0;
						
						// Add the last message to the last request
						// If the last request is too big, create a new request
						lastRequest.push(LAST_BIG_MESSAGE);
						for (let contextFile of contextFiles) {
							i += 1;
							let fileContent = readFileSync(contextFile, 'utf8');
							let contextMessage = { role: 'system', content: 'Here is the your full report number ' + i + ' : ' + JSON.stringify([fileContent]) };
							lastRequest.push(contextMessage);
						}
						
						// Count the number of tokens in the message with a python script
						for (let messages of lastRequest) {
							let messageFile = fileSync();
							writeFileSync(messageFile.name, JSON.stringify([messages]));
							// console.log('Message file : ', JSON.stringify([messages]));
							let stdout = (await exec(`python3 tokenCounter.py "${messageFile.name}"`)).stdout;
							contextRequestSize += Number(stdout);
						}
						
						if (contextRequestSize  < MAX_TOKENS) {
							console.log(chalk.magenta('Sending instructions (' + lastRequest.length + ' messages )'));
							const response = await openai.chat.completions.create({
								model: 'gpt-3.5-turbo',
								messages: lastRequest
							});

							// Generating the README
							writeFileSync(join(__dirname, projectPath, 'README.md'), response.choices[0].message.content + footer);

							// Update the overview variables
							tokensUsed += Number(response.usage.total_tokens);
							price = Number((tokensUsed/1000)*0.0005);
						} else {
							console.log(chalk.yellow('Too big request, the generation of the README will be stopped.'));
							rl.close();
						}

						// Display the overview
						console.log("------------------")
						console.log('README generated in : ' , chalk.green(join(__dirname, projectPath, 'README.md')));
						console.log('Tokens used : ', tokensUsed , '( ' + chalk.red('+- ' + price.toFixed(3) + ' $') + ' )');
					} else {
						// If it's a long request, wait 1 minute to respect the tpm
						if (longRequest && i % requestPerMin === 0) {
							console.log(chalk.cyan(requestPerMin + ' requests sent, waiting 1 minute to respect the tpm...'));

							// Wait 1 minute
							await new Promise((resolve) => {
								setTimeout(resolve, 60000);
							});
						}
					}
					i += 1;
				}
			} else {
				console.log(chalk.yellow('Requests not sent.'));
			}
		} 
		// If the request is not too big
		else {
			// Sending the request to OpenAI
			console.log('Sending request to OpenAI...');
			const response = await openai.chat.completions.create({
				model: 'gpt-3.5-turbo',
				messages: messages
			});

			// Update the overview variables
			const tokensUsed = response.usage.total_tokens;
			const price = (tokensUsed/1000)*0.0005;

			// Generating the README
			writeFileSync(join(__dirname, projectPath, 'README.md'), response.choices[0].message.content + footer);

			// Display the overview
			console.log("------------------")
			console.log('README generated in : ' , chalk.green(join(__dirname, projectPath, 'README.md')));
			console.log('Tokens used : ', tokensUsed , '( ' + chalk.red('+- ' + price.toFixed(3) + ' $') + ' )');
		}
	} else {
		console.log(chalk.yellow('Request not sent.'));
	}

	// Close the readline interface
	rl.close();
}


// *------------------------------------*
// |                                    |
// |           MAIN PROGRAM             |
// |                                    |
// *------------------------------------*

// get all files (- avoid) in the project directory
processDirectory(projectPath, avoid);

console.log(chalk.bold('Files to process : '), fileStack);

// process all files
Promise.all(fileStack.map(element => processFile(element)))
.then(() => { sendRequest(projectPath, messageList); })
.catch(err => console.error(err));